{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6RtFDqwe_-oh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.models.segmentation.deeplabv3 as dlv3\n",
        "import torchvision.transforms.functional as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import utils\n",
        "import data\n",
        "import copy\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import Subset\n",
        "from torchsummary import summary\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm import tqdm\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAHxPFi3_-on"
      },
      "source": [
        "## Training-related functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IlpRZU8c_-on"
      },
      "outputs": [],
      "source": [
        "def validate_epoch(model, device, val_dataloader, loss_function, num_classes, ignore_label):\n",
        "    model.eval()\n",
        "    batch_losses = []\n",
        "    metrics = {'miou': 0, 'ious': np.zeros(shape=(num_classes,), dtype=np.float64), 'loss': 0}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for (inputs, targets) in val_dataloader:\n",
        "            ipts = inputs.to(device, non_blocking=True)# torch.autograd.Variable(inputs).cuda()\n",
        "            tgts = targets.to(device, non_blocking=True)# torch.autograd.Variable(targets).cuda()\n",
        "            \n",
        "            preds = model(ipts)['out']\n",
        "            loss = loss_function(preds, tgts.squeeze(1).long())\n",
        "            loss_val = loss.item()\n",
        "            batch_losses += [loss_val]\n",
        "\n",
        "            preds = torch.argmax(preds.cpu(), dim=1)\n",
        "            tgts = torch.squeeze(targets, dim=1)\n",
        "\n",
        "            miou, ious = utils.measure_performance(preds, tgts, num_classes=num_classes, ignore_label=ignore_label)\n",
        "\n",
        "            metrics['ious'] += ious\n",
        "            metrics['miou'] += miou\n",
        "            metrics['loss'] += loss_val\n",
        "\n",
        "        metrics['ious'] /= float(len(val_dataloader))\n",
        "        metrics['miou'] /= float(len(val_dataloader))\n",
        "        metrics['loss'] /= float(len(val_dataloader))\n",
        "        \n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HCkzs16c_-oo"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, device, scaler, train_dataloader, loss_function, optimizer):\n",
        "    model.train()\n",
        "    batch_losses = []\n",
        "    \n",
        "    for (inputs, targets) in train_dataloader:\n",
        "        ipts = inputs.to(device, non_blocking=True)# torch.autograd.Variable(inputs).cuda()\n",
        "        tgts = targets.to(device, non_blocking=True).squeeze(1).long() #tgts = torch.autograd.Variable(targets).cuda().squeeze(1).long()\n",
        "        pred = model(ipts)['out']\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            loss = loss_function(pred, tgts)\n",
        "            loss_val = loss.item()\n",
        "            batch_losses += [loss_val]\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        scaler.scale(loss).backward() # loss.backward()\n",
        "        scaler.step(optimizer) #optimizer.step()\n",
        "        scaler.update()\n",
        "\n",
        "    return batch_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BQIfP6W3_-oo"
      },
      "outputs": [],
      "source": [
        "def train(model, device, scaler, train_dataloader, val_dataloader, epochs, loss_function, optimizer, lr_initial, lr_decay, num_classes_val, ignore_label):\n",
        "    model.train()\n",
        "\n",
        "    epoch_train_losses = []\n",
        "    epoch_val_losses = []\n",
        "    epoch_val_mious = []\n",
        "    epoch_val_ious = []\n",
        "\n",
        "    best_model = copy.deepcopy(model.state_dict())\n",
        "    best_val_miou = 0\n",
        "    \n",
        "    for epoch in tqdm(range(epochs), desc='Epoch progress'):\n",
        "\n",
        "        # Training phase\n",
        "        batch_train_losses = train_epoch(\n",
        "            model=model, \n",
        "            device=device,\n",
        "            scaler=scaler,\n",
        "            train_dataloader=train_dataloader,\n",
        "            loss_function=loss_function, \n",
        "            optimizer=optimizer)\n",
        "        \n",
        "        # Validation phase\n",
        "        batch_val_metrics = validate_epoch(\n",
        "            model=model,\n",
        "            device=device,\n",
        "            val_dataloader=val_dataloader,\n",
        "            loss_function=loss_function,\n",
        "            num_classes=num_classes_val,\n",
        "            ignore_label=ignore_label)\n",
        "\n",
        "        epoch_average_train_loss = np.mean(batch_train_losses)\n",
        "        epoch_train_losses += [epoch_average_train_loss]\n",
        "        epoch_val_losses += [batch_val_metrics['loss']]\n",
        "        epoch_val_mious += [batch_val_metrics['miou']]\n",
        "        epoch_val_ious += [batch_val_metrics['ious'].tolist()]\n",
        "\n",
        "        print(f'\\n[TRAIN] Epoch average loss: {epoch_average_train_loss:.4f}')\n",
        "        print(f'[VAL] Epoch average loss: {batch_val_metrics[\"loss\"]:.4f}')\n",
        "        print(f'[VAL] Epoch average miou: {100 * batch_val_metrics[\"miou\"]:.2f}%')\n",
        "        \n",
        "        if lr_decay:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr_initial * ((1.0 - (float(epoch) / float(epochs))) ** 0.9)\n",
        "\n",
        "        if epoch == 0 or batch_val_metrics['miou'] > best_val_miou:\n",
        "            best_val_miou = batch_val_metrics['miou']\n",
        "            best_model = copy.deepcopy(model.state_dict())\n",
        "            print('[MODEL] Checkpoint saved.\\n')\n",
        "\n",
        "    model.load_state_dict(best_model)\n",
        "    return model, {'epoch_train_losses': epoch_train_losses, 'epoch_val_losses': epoch_val_losses, 'epoch_val_mious': epoch_val_mious, 'epoch_val_ious': epoch_val_ious}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rc2ZjUV_-oo"
      },
      "source": [
        "## Training configuration loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q45lVAz2_-op",
        "outputId": "ed87cac9-6d68-4a33-9c9c-096a0a092c82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'data_batch_size': 4,\n",
            " 'data_source': 'real',\n",
            " 'data_subset_size': 16,\n",
            " 'epochs': 4,\n",
            " 'fine_tune': False,\n",
            " 'fine_tune_model_path': 'G:/My Drive/Master IVA/Master '\n",
            "                         'Thesis/Models/20230403_210427/deeplabv3_model.pt',\n",
            " 'ignore_label': 19,\n",
            " 'image_height': 256,\n",
            " 'image_width': 256,\n",
            " 'learning_rate': 0.0005,\n",
            " 'learning_rate_paper_decay': True,\n",
            " 'model_save_path': 'G:/My Drive/Master IVA/Master Thesis/Models',\n",
            " 'val_data_batch_size': 4,\n",
            " 'val_data_subset_size': 8}\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "args = {}\n",
        "with open('G:/My Drive/Master IVA/Master Thesis/Configs/train-config-2.json') as json_file:\n",
        "    args = json.load(json_file)\n",
        "pprint.pprint(args)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data loading and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l3c4vosw_-op"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "train_dataset = data.HybridDataset(root_path=f'G:/My Drive/Master IVA/Master Thesis/Datasets/{args[\"data_source\"]}/train',\n",
        "                                   input_dir='rgb',\n",
        "                                   target_dir='semantic_segmentation_mapped',\n",
        "                                   ipt_transform=torchvision.transforms.Compose([torchvision.transforms.Resize((args[\"image_height\"], args[\"image_width\"]))]),\n",
        "                                   tgt_transform=torchvision.transforms.Compose([torchvision.transforms.Resize((args[\"image_height\"], args[\"image_width\"]))]),\n",
        "                                   type=args[\"data_source\"],\n",
        "                                   labels_mapping=None)\n",
        "train_dataset = Subset(train_dataset, indices=range(args[\"data_subset_size\"]))\n",
        "train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                              batch_size=args[\"data_batch_size\"],\n",
        "                              shuffle=True)\n",
        "val_dataset = data.HybridDataset(root_path=f'G:/My Drive/Master IVA/Master Thesis/Datasets/{args[\"data_source\"]}/val',\n",
        "                                 input_dir='rgb',\n",
        "                                 target_dir='semantic_segmentation_mapped',\n",
        "                                 ipt_transform=torchvision.transforms.Compose([torchvision.transforms.Resize((args[\"image_height\"], args[\"image_width\"]))]),\n",
        "                                 tgt_transform=torchvision.transforms.Compose([torchvision.transforms.Resize((args[\"image_height\"], args[\"image_width\"]))]),\n",
        "                                 type=args[\"data_source\"],\n",
        "                                 labels_mapping=None)\n",
        "val_dataset = Subset(val_dataset, indices=range(args[\"val_data_subset_size\"]))\n",
        "val_dataloader = DataLoader(dataset=val_dataset,\n",
        "                            batch_size=args[\"val_data_batch_size\"],\n",
        "                            shuffle=True)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdJK3pF7RRKn",
        "outputId": "91ef22bf-48a8-43c4-acf3-6e9701143f6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training dataset norm. params. comp. progress: 100%|██████████| 4/4 [00:01<00:00,  2.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset 'mean':tensor([0.2366, 0.2782, 0.2368])\n",
            "Train dataset 'std deviation':tensor([0.1392, 0.1485, 0.1444])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation dataset norm. params. comp. progress:   0%|          | 0/2 [00:00<?, ?it/s]C:\\Users\\Manuel\\AppData\\Local\\Temp\\ipykernel_10624\\3029786214.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  val_inputs_tensor = torch.tensor(val_inputs)\n",
            "Validation dataset norm. params. comp. progress: 100%|██████████| 2/2 [00:00<00:00,  2.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation dataset 'mean':tensor([0.3475, 0.3759, 0.3380])\n",
            "Validation dataset 'std deviation':tensor([0.2413, 0.2410, 0.2390])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_pixel_sum = torch.tensor([0.0, 0.0, 0.0])\n",
        "train_pixel_sum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
        "\n",
        "for train_inputs, _ in tqdm(train_dataloader, desc='Training dataset norm. params. comp. progress'):\n",
        "    train_pixel_sum += train_inputs.sum(axis = [0, 2, 3])\n",
        "    train_pixel_sum_sq += (train_inputs ** 2).sum(axis = [0, 2, 3])\n",
        "\n",
        "train_pixel_count = args[\"data_subset_size\"] * args[\"image_width\"] * args[\"image_height\"]\n",
        "train_mean = train_pixel_sum / train_pixel_count\n",
        "train_variance = (train_pixel_sum_sq / train_pixel_count) - (train_mean ** 2)\n",
        "train_std = torch.sqrt(train_variance)\n",
        "\n",
        "print(f'Train dataset \\'mean\\':{train_mean}')\n",
        "print(f'Train dataset \\'std deviation\\':{train_std}')\n",
        "\n",
        "val_pixel_sum = torch.tensor([0.0, 0.0, 0.0])\n",
        "val_pixel_sum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
        "\n",
        "for val_inputs, _ in tqdm(val_dataloader, desc='Validation dataset norm. params. comp. progress'):\n",
        "    val_inputs_tensor = torch.tensor(val_inputs)\n",
        "    val_pixel_sum += val_inputs_tensor.sum(axis = [0, 2, 3])\n",
        "    val_pixel_sum_sq += (val_inputs_tensor ** 2).sum(axis = [0, 2, 3])\n",
        "\n",
        "val_pixel_count = args[\"val_data_subset_size\"] * args[\"image_width\"] * args[\"image_height\"]\n",
        "val_mean = val_pixel_sum / val_pixel_count\n",
        "val_variance = (val_pixel_sum_sq / val_pixel_count) - (val_mean ** 2)\n",
        "val_std = torch.sqrt(val_variance)\n",
        "\n",
        "print(f'Validation dataset \\'mean\\':{val_mean}')\n",
        "print(f'Validation dataset \\'std deviation\\':{val_std}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M8kQ9HkVRRKo"
      },
      "outputs": [],
      "source": [
        "train_dataset = data.HybridDataset(root_path=f'G:/My Drive/Master IVA/Master Thesis/Datasets/{args[\"data_source\"]}/train',\n",
        "                                   input_dir='rgb',\n",
        "                                   target_dir='semantic_segmentation_mapped',\n",
        "                                   ipt_transform=torchvision.transforms.Compose([torchvision.transforms.Resize((args[\"image_height\"], args[\"image_width\"])),\n",
        "                                                                                 torchvision.transforms.Normalize(mean=list(train_mean), std=list(train_std))]),\n",
        "                                   tgt_transform=torchvision.transforms.Compose([torchvision.transforms.Resize((args[\"image_height\"], args[\"image_width\"]))]),\n",
        "                                   type=args[\"data_source\"],\n",
        "                                   labels_mapping=None)\n",
        "train_dataset = Subset(train_dataset, indices=range(args[\"data_subset_size\"]))\n",
        "train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                              batch_size=args[\"data_batch_size\"],\n",
        "                              shuffle=True, pin_memory=True, num_workers=4)\n",
        "val_dataset = data.HybridDataset(root_path=f'G:/My Drive/Master IVA/Master Thesis/Datasets/{args[\"data_source\"]}/val',\n",
        "                                 input_dir='rgb',\n",
        "                                 target_dir='semantic_segmentation_mapped',\n",
        "                                 ipt_transform=torchvision.transforms.Compose([torchvision.transforms.Resize((args[\"image_height\"], args[\"image_width\"])),\n",
        "                                                                               torchvision.transforms.Normalize(mean=list(val_mean), std=list(val_std))]),\n",
        "                                 tgt_transform=torchvision.transforms.Compose([torchvision.transforms.Resize((args[\"image_height\"], args[\"image_width\"]))]), \n",
        "                                 type=args[\"data_source\"],\n",
        "                                 labels_mapping=None)\n",
        "val_dataset = Subset(val_dataset, indices=range(args[\"val_data_subset_size\"]))\n",
        "val_dataloader = DataLoader(dataset=val_dataset,\n",
        "                            batch_size=args[\"val_data_batch_size\"],\n",
        "                            shuffle=True, pin_memory=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQIRQ39o_-op"
      },
      "source": [
        "## Training preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhR4VbF2_-op",
        "outputId": "f4675100-719a-425f-a436-2f27f3e405c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model to device.\n"
          ]
        }
      ],
      "source": [
        "model = dlv3.deeplabv3_resnet50(weights=None, output_stride=16, progress=True, num_classes=len(data.SemanticLabelMapper.ID_TO_STRING['common'].keys()))\n",
        "if args[\"fine_tune\"]:\n",
        "    model.load_state_dict(torch.load(args[\"fine_tune_model_path\"]))\n",
        "    for name, param in model.backbone.named_parameters():\n",
        "        if 'layer1' in name or 'layer2' in name or 'layer3' in name or 'layer4' in name or 'layer5' in name:\n",
        "            param.requires_grad = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print('Loaded model to device.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ud3YI0lQ_-oq"
      },
      "outputs": [],
      "source": [
        "params = utils.add_weight_decay(model, l2_value=0.0001)\n",
        "optimizer = torch.optim.Adam(params=params, lr=args[\"learning_rate\"])\n",
        "loss_function = nn.CrossEntropyLoss()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VKyjxND_-oq",
        "outputId": "ac9f3984-73d5-47ed-9f42-a93c40938b73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch progress:   0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(17,)\n",
            "(17,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch progress:  25%|██▌       | 1/4 [00:09<00:28,  9.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[TRAIN] Epoch average loss: 2.1415\n",
            "[VAL] Epoch average loss: 1.7197\n",
            "[VAL] Epoch average miou: 9.35%\n",
            "[MODEL] Checkpoint saved.\n",
            "\n",
            "(17,)\n",
            "(17,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch progress:  50%|█████     | 2/4 [00:16<00:15,  7.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[TRAIN] Epoch average loss: 1.2538\n",
            "[VAL] Epoch average loss: 1.1941\n",
            "[VAL] Epoch average miou: 13.34%\n",
            "[MODEL] Checkpoint saved.\n",
            "\n",
            "(17,)\n",
            "(17,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch progress:  75%|███████▌  | 3/4 [00:23<00:07,  7.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[TRAIN] Epoch average loss: 0.9576\n",
            "[VAL] Epoch average loss: 0.9725\n",
            "[VAL] Epoch average miou: 16.64%\n",
            "[MODEL] Checkpoint saved.\n",
            "\n",
            "(17,)\n",
            "(17,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch progress: 100%|██████████| 4/4 [00:30<00:00,  7.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[TRAIN] Epoch average loss: 0.8333\n",
            "[VAL] Epoch average loss: 0.9064\n",
            "[VAL] Epoch average miou: 18.16%\n",
            "[MODEL] Checkpoint saved.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "torch.backends.cudnn.benchmark = True\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "model, train_metrics = train(model=model,\n",
        "                             device=device,\n",
        "                             scaler=scaler,\n",
        "                             train_dataloader=train_dataloader, \n",
        "                             val_dataloader=val_dataloader, \n",
        "                             epochs=args[\"epochs\"], \n",
        "                             loss_function=loss_function, \n",
        "                             optimizer=optimizer, \n",
        "                             lr_initial=args[\"learning_rate\"],\n",
        "                             lr_decay=args[\"learning_rate_paper_decay\"],\n",
        "                             num_classes_val=len(data.SemanticLabelMapper.ID_TO_STRING['common'].keys()),\n",
        "                             ignore_label=None)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train report generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1sEVMrQz_-oq"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Object of type ndarray is not JSON serializable",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Manuel\\Projects\\GitHub_Repositories\\master_thesis\\python_scripts_semseg\\train.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Manuel/Projects/GitHub_Repositories/master_thesis/python_scripts_semseg/train.ipynb#X23sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodel_save_path\u001b[39m}\u001b[39;00m\u001b[39m/train_config.json\u001b[39m\u001b[39m'\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Manuel/Projects/GitHub_Repositories/master_thesis/python_scripts_semseg/train.ipynb#X23sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     f\u001b[39m.\u001b[39mwrite(args)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Manuel/Projects/GitHub_Repositories/master_thesis/python_scripts_semseg/train.ipynb#X23sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m train_metrics \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mdumps(train_metrics, indent\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Manuel/Projects/GitHub_Repositories/master_thesis/python_scripts_semseg/train.ipynb#X23sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmodel_save_path\u001b[39m}\u001b[39;00m\u001b[39m/train_metrics.json\u001b[39m\u001b[39m'\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Manuel/Projects/GitHub_Repositories/master_thesis/python_scripts_semseg/train.ipynb#X23sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     f\u001b[39m.\u001b[39mwrite(train_metrics)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\json\\__init__.py:234\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONEncoder\n\u001b[1;32m--> 234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[0;32m    235\u001b[0m     skipkeys\u001b[39m=\u001b[39;49mskipkeys, ensure_ascii\u001b[39m=\u001b[39;49mensure_ascii,\n\u001b[0;32m    236\u001b[0m     check_circular\u001b[39m=\u001b[39;49mcheck_circular, allow_nan\u001b[39m=\u001b[39;49mallow_nan, indent\u001b[39m=\u001b[39;49mindent,\n\u001b[0;32m    237\u001b[0m     separators\u001b[39m=\u001b[39;49mseparators, default\u001b[39m=\u001b[39;49mdefault, sort_keys\u001b[39m=\u001b[39;49msort_keys,\n\u001b[0;32m    238\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\u001b[39m.\u001b[39;49mencode(obj)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\json\\encoder.py:201\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    199\u001b[0m chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterencode(o, _one_shot\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(chunks, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m--> 201\u001b[0m     chunks \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(chunks)\n\u001b[0;32m    202\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(chunks)\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\json\\encoder.py:431\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[0;32m    430\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(o, \u001b[39mdict\u001b[39m):\n\u001b[1;32m--> 431\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[0;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     \u001b[39mif\u001b[39;00m markers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\json\\encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    404\u001b[0m             chunks \u001b[39m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 405\u001b[0m         \u001b[39myield from\u001b[39;00m chunks\n\u001b[0;32m    406\u001b[0m \u001b[39mif\u001b[39;00m newline_indent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     _current_indent_level \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\json\\encoder.py:325\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[1;34m(lst, _current_indent_level)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m             chunks \u001b[39m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 325\u001b[0m         \u001b[39myield from\u001b[39;00m chunks\n\u001b[0;32m    326\u001b[0m \u001b[39mif\u001b[39;00m newline_indent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    327\u001b[0m     _current_indent_level \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\json\\encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCircular reference detected\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    437\u001b[0m     markers[markerid] \u001b[39m=\u001b[39m o\n\u001b[1;32m--> 438\u001b[0m o \u001b[39m=\u001b[39m _default(o)\n\u001b[0;32m    439\u001b[0m \u001b[39myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[0;32m    440\u001b[0m \u001b[39mif\u001b[39;00m markers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\json\\encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault\u001b[39m(\u001b[39mself\u001b[39m, o):\n\u001b[0;32m    161\u001b[0m     \u001b[39m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[39m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mObject of type \u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    180\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mis not JSON serializable\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;31mTypeError\u001b[0m: Object of type ndarray is not JSON serializable"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import datetime\n",
        "now = datetime.datetime.now()\n",
        "current_time = now.strftime(\"%Y%m%d_%H%M%S\")\n",
        "model_save_path = f'{args[\"model_save_path\"]}/{current_time}'\n",
        "os.mkdir(model_save_path)\n",
        "\n",
        "matplotlib.rcParams.update({'font.size': 6})\n",
        "\n",
        "plt.plot(train_metrics['epoch_train_losses'], label='Train loss', color='blue') \n",
        "plt.plot(train_metrics['epoch_val_losses'], label='Validation loss', color='yellow') \n",
        "plt.xlabel('Epoch') \n",
        "plt.xticks(ticks=np.arange(0, args['epochs']))\n",
        "plt.ylabel('Loss') \n",
        "plt.title('Loss over Epochs') \n",
        "plt.legend()\n",
        "plt.savefig(f'{model_save_path}/loss_over_epochs.jpg', dpi=1200)\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(train_metrics['epoch_val_mious'], label='Validation mIoU', color='green') \n",
        "plt.xlabel('Epoch') \n",
        "plt.xticks(ticks=np.arange(0, args['epochs']))\n",
        "plt.ylabel('mIoU') \n",
        "plt.yticks(np.arange(0.0, 1.1, 0.1))\n",
        "plt.title('mIoU over Epochs') \n",
        "plt.legend()\n",
        "plt.savefig(f'{model_save_path}/iou_over_epochs.jpg', dpi=1200)\n",
        "\n",
        "train_std = train_std.tolist()\n",
        "train_mean = train_mean.tolist()\n",
        "\n",
        "eval_args = {'model_load_folder_path': model_save_path,\n",
        "             'train_std': train_std,\n",
        "             'train_mean': train_mean,\n",
        "             'data_source': \"real\",\n",
        "             'test_subset_size': 300,\n",
        "             'test_batch_size': 4,\n",
        "             'ignore_label': args['ignore_label'],\n",
        "             'image_width': args['image_width'],\n",
        "             'image_height': args['image_height']}\n",
        "eval_args = json.dumps(eval_args, indent=4)\n",
        "with open(file=f'{model_save_path}/eval-config.json', mode='w') as f:\n",
        "    f.write(eval_args)\n",
        "    \n",
        "args = json.dumps(args, indent=4)\n",
        "with open(file=f'{model_save_path}/train_config.json', mode='w') as f:\n",
        "    f.write(args)\n",
        "\n",
        "train_metrics = json.dumps(train_metrics, indent=4)\n",
        "with open(file=f'{model_save_path}/train_metrics.json', mode='w') as f:\n",
        "    f.write(train_metrics)\n",
        "\n",
        "torch.save(model.state_dict(), f'{model_save_path}/deeplabv3_model.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
